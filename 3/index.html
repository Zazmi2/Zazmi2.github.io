<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CS180: Image Mosaicing & Autostitching</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      margin: 0;
      padding: 0;
      background: url('background.jpg') no-repeat center center fixed;
      background-size: cover;
      color: #fff;
    }
    .container {
      max-width: 1000px;
      margin: auto;
      padding: 40px;
      background: rgba(0, 0, 0, 0.7);
      border-radius: 15px;
      box-shadow: 0 8px 20px rgba(0,0,0,0.5);
    }
    h1, h2, h3 {
      text-align: center;
    }
    .logo {
      display: block;
      margin: 20px auto;
      width: 120px;
    }
    section {
      margin-bottom: 50px;
    }
    .images {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      justify-content: center;
    }
    .images img {
      max-width: 300px;
      border-radius: 10px;
      box-shadow: 0 4px 10px rgba(0,0,0,0.4);
    }
    p {
      max-width: 800px;
      margin: 10px auto;
      line-height: 1.6;
    }
    pre {
      background: rgba(255,255,255,0.1);
      padding: 10px;
      border-radius: 8px;
      overflow-x: auto;
      color: #ddd;
    }
    hr {
      border: 1px solid rgba(255,255,255,0.2);
      margin: 40px 0;
    }
  </style>
</head>

<body>
  <div class="container">
    <img src="outputimgs/berkeley_logo.png" alt="UC Berkeley Logo" class="logo">
    <h1>CS180/280A: Intro to Computer Vision and Computational Photography</h1>
    <h2>Programming Project #3 (proj3B)</h2>
    <h3>Image Mosaicing and Autostitching</h3>

    <hr>

    <!-- ================= PART A ================= -->
    <section id="a1">
      <h2>A.1: Shoot and Digitize Pictures</h2>
      <p>Here are the sets of images with projective transformations between them. These images were captured by rotating the camera around a fixed center of projection.</p>
      <div class="images">
        <img src="outputimgs/rock_pic1.jpg" alt="Set 1 - Image 1">
        <img src="outputimgs/rock_pic2.jpg" alt="Set 1 - Image 2">
        <img src="outputimgs/pic_2.jpg" alt="Set 2 - Image 1">
        <img src="outputimgs/pic_3.jpg" alt="Set 2 - Image 2">
        <img src="outputimgs/mse_1.jpg" alt="Set 3 - Image 1">
        <img src="outputimgs/mse_2.jpg" alt="Set 3 - Image 2">
      </div>
    </section>

    <section id="a2">
      <h2>A.2: Recover Homographies</h2>
      <p>Point correspondences were manually selected. Below are visualizations of correspondences, the system of equations, and the recovered homography matrices.</p>
      <div class="images">
        <div>
          <img src="outputimgs/PointtoPointCorrespondences1.png" alt="Correspondences North Berkeley">
          <pre>
[[ 2.05716515e+00  4.79193132e-02 -1.72646035e+03]
 [ 2.77236115e-01  1.62783748e+00 -6.53745347e+02]
 [ 2.52174387e-04  2.24894471e-05  1.00000000e+00]]
          </pre>
        </div>
        <div>
          <img src="outputimgs/PointtoPointCorrespondences2.png" alt="Correspondences Mining Building">
          <pre>
[[ 1.92839578e+00 -2.06904725e-01 -2.89045568e+03]
 [ 3.50907364e-01  1.43354737e+00 -1.04948836e+03]
 [ 2.45250478e-04 -1.08486579e-04  1.00000000e+00]]
          </pre>
        </div>
        <div>
          <img src="outputimgs/PointtoPointCorrespondences3.png" alt="Correspondences Rock Pics">
          <pre>
[[ 4.72349376e-01  1.20889417e+00 -7.79746557e+02]
 [ 8.61444746e-01 -8.10771526e-01  2.17762973e+02]
 [-7.34380019e-05  3.47289536e-04  1.00000000e+00]]
          </pre>
        </div>
      </div>
    </section>

    <section id="a3">
      <h2>A.3: Warp the Images</h2>
      <p>Images were warped using both nearest-neighbor and bilinear interpolation. Comparisons between the two methods are shown below. Rectification examples are also included.</p>
      <div class="images">
        <img src="outputimgs/phone.jpg" alt="Original Image 1">
        <img src="outputimgs/phone_fixed_NearestNeighbor.jpg" alt="Nearest Neighbor Warp 1">
        <img src="outputimgs/phone_fixed_Bilinear.jpg" alt="Bilinear Warp 1">
        <img src="outputimgs/laptop.jpg" alt="Original Image 2">
        <img src="outputimgs/laptop_fixed_NearestNeighbor.jpg" alt="Nearest Neighbor Warp 2">
        <img src="outputimgs/laptop_fixed_bilinear.jpg" alt="Bilinear Warp 2">
      </div>
    </section>

    <section id="a4">
      <h2>A.4: Blend Images into a Mosaic</h2>
      <p>The images were blended into mosaics using weighted averaging to reduce edge artifacts. Here are three mosaics with their corresponding source images.</p>
      <div class="images">
        <img src="outputimgs/pic_2.jpg" alt="Mosaic 1 Source Image">
        <img src="outputimgs/pic_3.jpg" alt="Mosaic 1 Source Image">
        <img src="outputimgs/panaroma_1.png" alt="Mosaic 1">
        <img src="outputimgs/mse_1.jpg" alt="Mosaic 2 Source Image">
        <img src="outputimgs/mse_2.jpg" alt="Mosaic 2 Source Image">
        <img src="outputimgs/panaroma_2.png" alt="Mosaic 2">
        <img src="outputimgs/rock_pic1.jpg" alt="Mosaic 3 Source Image">
        <img src="outputimgs/rock_pic2.jpg" alt="Mosaic 3 Source Image">
        <img src="outputimgs/panorma3.png" alt="Mosaic 3">
      </div>
    </section>

    <hr>

    <!-- ================= PART B ================= -->
    <section id="b0">
      <h2>B: Feature Matching for Autostitching</h2>
      <p>The goal of this section is to automatically stitch images into a mosaic following the paper 
      <i>“Multi-Image Matching using Multi-Scale Oriented Patches”</i> by Brown et al., with several simplifications.</p>
    </section>

    <section id="b1">
      <h2>B.1: Harris Corner Detection</h2>
      <p>Implemented Harris Corner Detector and Adaptive Non-Maximal Suppression (ANMS) to obtain a uniform distribution of keypoints across the image. This is an example image that shows the results of Harris Corner detection followed by ANMS</p>
      <div class="images">
        <img src="outputimgs/anms_corners.png" alt="Harris Corners">
      </div>
    </section>

    <section id="b2">
      <h2>B.2: Feature Descriptor Extraction</h2>
      <p>Extracted normalized 8x8 feature descriptors from 40x40 image patches centered at corner points. Each descriptor was bias/gain-normalized to ensure illumination invariance. Below shows five of the corners from the image that was showed in the section above.</p>
      <div class="images">
        <img src="outputimgs/feature_extractions_5.png" alt="Sample Feature Descriptors">
      </div>
    </section>

    <section id="b3">
      <h2>B.3: Feature Matching</h2>
      <p>Used vectorized pairwise L2 distance computation between descriptors and applied Lowe’s ratio test to identify reliable correspondences between image pairs. Below are the matching correspondences from all the mosaics I eventually sitch together</p>
      <div class="images">
        <img src="outputimgs/pic2_3_matchted_features.png" alt="Matched Features img 1">
        <img src="outputimgs/rockpic2_1_matched2_1.png" alt="Matched Features img 2">
        <img src="outputimgs/car1_2_matched_features.png" alt="Matched Features img 3">
      </div>
    </section>

    <section id="b4">
      <h2>B.4: RANSAC for Robust Homography</h2>
      <p>Implemented 4-point RANSAC to estimate homographies robustly against outliers. The resulting mosaics are automatically stitched using these computed transformations. I had to change one of my sets of images from the previous one (panoroma 2 to panaroma 4) since the Feature Extraction and matching the points between the two images wasn't accurate. I think this because the lighting affected the corner pixel intensities and it wasn't able to properly match the corners from each image together. I used a different set of images and it worked well since it was able to accuractly match the corners together well.</p>
      <div class="images">
        <img src="outputimgs/panaroma_1_ransacv2.png" alt="Automatic Mosaic 1">
        <img src="outputimgs/panaroma_3_ransac.jpg" alt="Automatic Mosaic 2">
        <img src="outputimgs/Panorma4_ransac_afterEdgeReductions.png" alt="Automatic Mosaic 3">
      </div>
    </section>

    <section id="part-differences">
    <h2>Differences Between Part A and Part B</h2>
    <p>
      The results between part a and part b ended up being very similar. Part b might have slightly better results because the corner detection and matching through ransac would be more accurate vs manually selecting points as I did in part a. Both methods used a simple blending process where I averaged the pixel values for the masks. This simple averaging method made it way better than just simpling stiching the two images together. To make the edge artifcats smoother, one can use laplcian blending as well.
    </p>
</section>
  </div>
</body>
</html>
